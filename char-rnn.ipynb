{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "altered-validity",
   "metadata": {},
   "source": [
    "# char-rnn in Training time\n",
    "![char-rnn for train time](./char-rnn-train.png)\n",
    "# char-rnn in Test time\n",
    "![char-rnn for test time](./char-rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-respondent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V', 'Q', 'H', 'w', 'v', 'h', '4', 'X', ')', 'S', '1', '\"', '*', 'L', 'Z', '7', '\\n', \"'\", 'b', 'q', 'a', 'O', '[', 'B', '=', '(', 'D', 'Y', 't', 'T', 'K', 'A', ',', 'R', 'F', 's', 'c', 'P', 'W', '.', '9', 'E', 'f', '?', 'C', '3', 'o', 'e', '-', 'r', 'g', ']', 'k', '0', 'I', '/', '6', 'p', 'U', 'j', ';', 'y', 'm', 'd', '5', '+', 'i', ' ', 'x', 'n', 'l', 'J', '8', 'N', 'z', '!', 'M', 'G', '2', 'u', ':']\n",
      "data has 588109 characters, 81 unique.\n",
      "----\n",
      " Cz*D'U,\n",
      "q5,d8AI -CK=fg=Rxx3f=3J9KD.?Z-f*v'i6c(Q4u4fT8vmVUW8zTc48ElRT,G=pNS4Kp[lm[ij:dtw7vm!R  90bRf6U,X98qF h)aa=Gz239XQCxg\"mkj4-uzB,x50\"mom?X\n",
      "XysymsX3!ndH.Y5t3 d.a=;?XifoQ,6:y1Rc\"XZU1oCaN'([8140GUqtC \n",
      "----\n",
      "iter 0, loss: 109.861239\n",
      "----\n",
      " h sn,oe r dalgdaf\n",
      "Nwtob hinutotMh SUst u.o nhaouyothih ttnrtA. uoi uo ghh\"  l, h)  Cnifpc sato esiapn O hofpherlfs h(X:p-ofag  xn \n",
      "utptN s toy cnf hpe  meauo sunoeine pW\n",
      "slchy nd uloa tnbyi  n esheotl \n",
      "----\n",
      "iter 100, loss: 110.322177\n",
      "----\n",
      "  dhonu s  wdauf  dvglmp\n",
      " sa  ovehcre yahcoulht  nga,tTng-hcd eo odh v,ath, tnldvyye caBhapuirNvlhayh eaea yng tdtoe-anfYheett dt  eyaem ez ki eahen ehewoueogwhu eadtrdarah rl e hkwd esefivo eo.enrtob. \n",
      "----\n",
      "iter 200, loss: 107.763165\n",
      "----\n",
      " ,  ud n c rbAcirwan ai wohreel o s  hOito.n shn cawbndat a\n",
      "oue\n",
      "e s aMs 1etn M fMhc Loa evh ce enne \n",
      "atnodh\n",
      " owMde o tn\n",
      "emer m- onsioce  soiapw eesaoSerio  izhiee-heisninokdownoys bdteiastiiee,cwsenaee \n",
      "----\n",
      "iter 300, loss: 105.362469\n",
      "----\n",
      "  bele , nk,iss epin ut hwmeecobale, , e rin beladnutle athia solin, axnN o attia tis b ot seublg\n",
      "etsteae  eos,nnin, bi,\n",
      "twe. ocgbvtoe o wtto lG nkbeubo wsiowong tgondwneasdnbior aa Anid t\n",
      "ohs thllltfo \n",
      "----\n",
      "iter 400, loss: 102.457048\n",
      "----\n",
      " lg taush ime\n",
      "ge teMor il\n",
      "\n",
      "ih wau sd btnoy eovas helt me tp ulfnaprNrep(is,gu\n",
      "dn wet vl\n",
      "leollhestho s\n",
      "de -it dpin:te ine\n",
      "wy thoa cen , dhe\n",
      "d poo  siletule ghasst w. txr.hgn cyye tlmurke the w\n",
      "he TWet r \n",
      "----\n",
      "iter 500, loss: 99.472647\n",
      "----\n",
      "  fsgdd thsnL\"m \n",
      "\n",
      "e  nltmgac ,tice de\n",
      "hunOdunin, \"pv f wedireathihin s ds\n",
      "tdte lor tsnernhy mom tel ds dbovt oe nhtr ptiv w)hh neu t Iu5 v hant ics  heec oy\n",
      ". , jnnipifhn\n",
      "deet osee  ha\n",
      ":8inlit doweuA9d \n",
      "----\n",
      "iter 600, loss: 96.937167\n",
      "----\n",
      " kn d asmmtoiun hero din yonoh soo couoeche t. n onifesn rhe woon or Won0.Vat meincoao(rtt fnann-ho an, an, oe fo ot an.nd b!o in coe ne hanmond pos ciina nod odar ano  yalnMhe ho coog tete at allvem - \n",
      "----\n",
      "iter 700, loss: 94.298316\n",
      "----\n",
      " rtredyer kethime paBwoapwn ane per 9owItImyiou\n",
      "cogss\n",
      "whin  h  ip-ein thibere woP iu cete te onn O bujs wha igopore re -nin to uis yht a.ore she r bift Sof a. hy anwnggren thaprel hegand wathabut o\n",
      "ili \n",
      "----\n",
      "iter 800, loss: 91.881436\n",
      "----\n",
      " oriklerk schecWpiserrs hisem te rga chir spts.t chn1omscherirscicrair lrgsmipfh. gg adas\n",
      "thmut\n",
      "therp p. tilce phes fsipih Hrecwang sfpg hesAprect f,s yoticthepwresibacor verphoc cslhasocpwpppretNcas c \n",
      "----\n",
      "iter 900, loss: 89.551237\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "print(chars)\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1 # One-hot vector\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "        #print('t is',t, ps[t], targets[t]), so above same as ps[t][targets[t]]\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. \n",
    "        #see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T) # here sum all dWhy, dby as used for every t\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1 #OneHot\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print( '----\\n %s \\n----' % (txt, ))\n",
    "    \n",
    "    #gradCheck(inputs, targets, hprev)\n",
    "    \n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print( 'iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "appreciated-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checking\n",
    "from random import uniform\n",
    "def gradCheck(inputs, target, hprev):\n",
    "    global Wxh, Whh, Why, bh, by\n",
    "    num_checks, delta = 10, 1e-5\n",
    "    _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)\n",
    "    for param,dparam,name in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                               ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
    "        s0 = dparam.shape\n",
    "        s1 = param.shape\n",
    "        assert s0 == s1, 'Error dims dont match: %s and %s.' % (s0, s1)\n",
    "        print(name)\n",
    "        for i in range(num_checks):\n",
    "            ri = int(uniform(0,param.size))\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            old_val = param.flat[ri]\n",
    "            param.flat[ri] = old_val + delta\n",
    "            cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "            param.flat[ri] = old_val - delta\n",
    "            cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "            param.flat[ri] = old_val # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            grad_analytic = dparam.flat[ri]\n",
    "            grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
    "            rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "            print( '%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))\n",
    "            # rel_error should be on order of 1e-7 or less"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
